{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "62cf2d2051c64df1a7bb95ad6ea6e11f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_577ef759739246fbb0b426d70c2f3c42",
              "IPY_MODEL_6fb4a51d16a44d49adcc8e0fab7abb02",
              "IPY_MODEL_eb355259ec2f42c2abe88041b6b0fadd"
            ],
            "layout": "IPY_MODEL_4c5f9e6ceb5b4960949ae016e2c101bf"
          }
        },
        "577ef759739246fbb0b426d70c2f3c42": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ef532e321f9448c906656534b9e53ec",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_85aa9d3da008404e86dbc6c1866135d5",
            "value": "Epochâ€‡99:â€‡100%"
          }
        },
        "6fb4a51d16a44d49adcc8e0fab7abb02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_93c8c4f339504542b441e547849a1101",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b09bb89a5d7d4f548d57ef7f18e10eb8",
            "value": 2
          }
        },
        "eb355259ec2f42c2abe88041b6b0fadd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f3bcb9d0f42f4075a00eda9b550f3962",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_e85b62f7ca40451492eb2564295ab9c8",
            "value": "â€‡2/2â€‡[00:00&lt;00:00,â€‡72.58it/s,â€‡v_num=1]"
          }
        },
        "4c5f9e6ceb5b4960949ae016e2c101bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "8ef532e321f9448c906656534b9e53ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85aa9d3da008404e86dbc6c1866135d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "93c8c4f339504542b441e547849a1101": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b09bb89a5d7d4f548d57ef7f18e10eb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f3bcb9d0f42f4075a00eda9b550f3962": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e85b62f7ca40451492eb2564295ab9c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### In this notebook, we will use PyTorch + Lightning to create and optimize a Decoder-Only Transformer, like the one shown in the picture below. Decoder-Only Transformers are taking over AI right now, and quite possibly their most famous use is in ChatGPT."
      ],
      "metadata": {
        "id": "YSldHvOu3b7H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Necessary Libraries"
      ],
      "metadata": {
        "id": "YnGSuHRC35gZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_P0uWCINOZJM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F #used to access softmax() function to calculate attention\n",
        "from torch.optim import Adam #used to fit neural network to the data for back propagation\n",
        "from torch.utils.data import TensorDataset, DataLoader #imported these modules to creat large scale transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pytorch-lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j-oo2UNPXNGZ",
        "outputId": "cded1d00-1f74-4c62-d400-6441fb4a52ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (2.5.2)\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (6.0.2)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\n",
            "Requirement already satisfied: torchmetrics>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (1.7.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.14.0)\n",
            "Requirement already satisfied: lightning-utilities>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (0.14.3)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.1.0->pytorch-lightning) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.1.0->pytorch-lightning) (1.3.0)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics>=0.7.0->pytorch-lightning) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.1.0->pytorch-lightning) (3.0.2)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (3.10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pytorch_lightning as L #used for automatic code optimization and scaling in the cloud"
      ],
      "metadata": {
        "id": "XBWQlXpuwIx0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create the input and output and data"
      ],
      "metadata": {
        "id": "GYUdrm8R6c3-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In this tutorial we will build a simple Decoder-Only Transformer that can answer two super simple questions, What is computer science? and computer science is what?, and give them both the same answer, Awesome!!!"
      ],
      "metadata": {
        "id": "l9l1XWs46jOV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### In order to keep track of our simple dataset, we'll create a dictionary that maps the words and tokens to ID numbers. This is because the class we will use to do word embedding for us, nn.Embedding(), only accepts ID numbers as input, rather than words or tokens."
      ],
      "metadata": {
        "id": "sRgaMpBy6vRM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_to_id= {'what':0,\n",
        "              'is':1,\n",
        "              'computer science':2,\n",
        "              'awesome':3,\n",
        "              '<EOS>':4\n",
        "              }"
      ],
      "metadata": {
        "id": "L1d5WStlwIvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_token = dict(map(reversed, token_to_id.items()))"
      ],
      "metadata": {
        "id": "8BPnLbfVwIsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs= torch.tensor([[token_to_id[\"what\"],\n",
        "                       token_to_id[\"is\"],\n",
        "                       token_to_id[\"computer science\"],\n",
        "                       token_to_id[\"<EOS>\"],\n",
        "                       token_to_id[\"awesome\"]],\n",
        "                      [token_to_id[\"computer science\"],\n",
        "                       token_to_id[\"is\"],\n",
        "                       token_to_id[\"what\"],\n",
        "                       token_to_id[\"<EOS>\"],\n",
        "                       token_to_id[\"awesome\"]]])"
      ],
      "metadata": {
        "id": "5LIfHQ8kwIqm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = torch.tensor([[token_to_id[\"is\"],\n",
        "                        token_to_id[\"computer science\"],\n",
        "                        token_to_id[\"<EOS>\"],\n",
        "                        token_to_id[\"awesome\"],\n",
        "                        token_to_id[\"<EOS>\"]],\n",
        "\n",
        "                       [token_to_id[\"is\"],\n",
        "                        token_to_id[\"what\"],\n",
        "                        token_to_id[\"<EOS>\"],\n",
        "                        token_to_id[\"awesome\"],\n",
        "                        token_to_id[\"<EOS>\"]]])"
      ],
      "metadata": {
        "id": "tb6I8K4WwInf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Then we will use the dictionary to create a Dataloader that contains the questions and the desired answers encoded as ID numbers. Ultimately we'll use the Dataloader to train the transformer. NOTE: Dataloaders are designed to scale to very large datasets, so this simple example should be useful even when you have a terabyte of text."
      ],
      "metadata": {
        "id": "ycyT_tv169wu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset= TensorDataset(inputs,labels)\n",
        "dataloader= DataLoader(dataset)"
      ],
      "metadata": {
        "id": "0pVqSaG2xTq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Position Encoding"
      ],
      "metadata": {
        "id": "pn4l1Y6q7QOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Position Encoding helps the transformer keep track of the order of the words in the input and the output."
      ],
      "metadata": {
        "id": "-_K-mrgH7S6o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "d_model is short for dimensions of the model which represents the number of word embedding values per token.\n",
        "\n",
        "max_len = maximum number of tokens we allow as input.\n",
        "\n",
        "Since we are precomputing the position encoding values and storing them in a lookup tablewe can use d_model and max_len to determine the number of rows and columns in that lookup table."
      ],
      "metadata": {
        "id": "WW8_gbpr8JFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionEncoding(nn.Module):\n",
        "  def __init__(self, d_model=2, max_len=6):\n",
        "    super().__init__()\n",
        "    ## We call the super's init because by creating our own __init__() method, we overwrite the one\n",
        "    #we inherited from nn.Module. So we have to explicity call nn.Module's __init__(), otherwise it\n",
        "    #won't get initialized.\n",
        "\n",
        "    #Now we create a lookup table, pe, of position encoding values and initialize all of them to 0.\n",
        "    #To do this, we will make a matrix of 0s that has max_len rows and d_model columns.\n",
        "    pe=torch.zeros(max_len, d_model)\n",
        "\n",
        "    #Now we create a sequence of numbers for each position that a token can have in the input (or output).\n",
        "    ## For example, if the input tokens where \"I'm happy today!\", then \"I'm\" would get the first\n",
        "    ## position, 0, \"happy\" would get the second position, 1, and \"today!\" would get the third position, 2.\n",
        "    ## NOTE: Since we are going to be doing math with these position indices to create the\n",
        "    ## positional encoding for each one, we need them to be floats rather than ints. We use torch.arange to create floats\n",
        "\n",
        "    ### Lastly, .unsqueeze(1) converts the single list of numbers that torch.arange creates into a matrix with\n",
        "    ## one row for each index, and all of the indices in a single column. So if \"max_len\" = 3, then we\n",
        "    ## would create a matrix with 3 rows and 1 column like this\n",
        "    position= torch.arange(start=0, end=max_len,step=1).float().unsqueeze(1)\n",
        "\n",
        "    ### The positional encoding equations used in \"Attention is all you need\" are.\n",
        "    ## PE(pos, 2i)   = sin(pos / 10000^(2i/d_model))\n",
        "    ## PE(pos, 2i+1) = cos(pos / 10000^(2i/d_model))\n",
        "    ## and we see, within the sin() and cos() functions, we divide \"pos\" by some number that depends\n",
        "    ## on the index (i) and total number of PE values we want per token (d_model).\n",
        "    ## NOTE: When the index, i, is 0 then we are calculating the y-axis coordinates on the **first pair**\n",
        "    ## of sine and cosine curves. When i=1, then we are calculating the y-axis coordiantes on the\n",
        "    ## **second pair** of sine and cosine curves. etc. etc.\n",
        "\n",
        "    embedding_index=torch.arange(start=0, end=d_model, step=2).float()\n",
        "\n",
        "    #Now let's create an index for the embedding positions to simplify the code a little more.\n",
        "    # Div term is originally calculated with the following formula:div_term = torch.exp(torch.arange(start=0, end=d_model, step=2).float() * -(math.log(10000.0) / d_model))\n",
        "    #However to problem with this is underflowing (getting close to 0), so to prevent this i wrapped everything in a call to torch.exp() and used torch.log to convert it to a tensor\n",
        "    div_term= 1/torch.tensor(10000.0)**(embedding_index/d_model)\n",
        "    pe[:,0::2]=torch.sin(position*div_term)\n",
        "    pe[:,1::2]=torch.cos(position*div_term)\n",
        "\n",
        "\n",
        "    #Now we \"register 'pe'.\n",
        "    self.register_buffer('pe',pe)\n",
        "    ## \"register_buffer()\" ensures that\n",
        "    ## 'pe' will be moved to wherever the model gets\n",
        "    ## moved to. So if the model is moved to a GPU, then,\n",
        "    ## even though we don't need to optimize 'pe', it will\n",
        "    ## also be moved to that GPU. This, in turn, means\n",
        "    ## that accessing 'pe' will be relatively fast copared\n",
        "    ## to having a GPU have to get the data from a CPU.\n",
        "\n",
        "\n",
        "# we will add the position encoding values to the word embedding values\n",
        "  def forward(self, word_embeddings):\n",
        "    return word_embeddings+ self.pe[:word_embeddings.size(0),:]"
      ],
      "metadata": {
        "id": "Ud-LsZGmxToR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### We're going to code an Attention class to do all of the types of attention that a transformer might need: Self-Attention, Masked Self-Attention (which is used by the Decoder during training), and Encoder-Decoder Attention.\n",
        "\n",
        "(Refer to Document for Matrix Math/ Softmax Function Interpertations and Visualisations provided in the Github Repository)"
      ],
      "metadata": {
        "id": "Jo_KvHypAm7F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention Class"
      ],
      "metadata": {
        "id": "QodgJRXxChpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Attention(nn.Module):\n",
        "  def __init__(self,d_model=2):\n",
        "    super().__init__()\n",
        "    self.d_model= d_model\n",
        "\n",
        "    ## Initialize the Weights (W) that we'll use to create the\n",
        "        ## query (q), key (k) and value (v) numbers for each token\n",
        "    self.query= nn.Linear(in_features=d_model, out_features=d_model, bias=False) #query matrix\n",
        "    self.key= nn.Linear(in_features=d_model, out_features=d_model, bias=False) #key matrix\n",
        "    self.value= nn.Linear(in_features=d_model, out_features=d_model, bias= False) # value matrix\n",
        "\n",
        "    ## NOTE: In this simple example, we are not training on the data in \"batches\"\n",
        "        ## However, by defining variables for row_dim and col_dim, we could\n",
        "        ## allow for batches by setting row_dim to 1 and col_com to 2.\n",
        "    self.row_dim=0\n",
        "    self.col_dim=1\n",
        "\n",
        "  def forward (self, encodings_for_q, encodings_for_k, encodings_for_v, mask=None):\n",
        "    ## Create the query, key and values using the encodings\n",
        "        ## associated with each token (token encodings)\n",
        "       ## NOTE: For Encoder-Decoder Attention, the encodings for q come from\n",
        "        ##the decoder and the encodings for k and v come from the output\n",
        "        ## from the encoder.\n",
        "    q=self.query(encodings_for_q)\n",
        "    k=self.key(encodings_for_k)\n",
        "    v=self.value(encodings_for_v)\n",
        "\n",
        "    ## Compute attention scores\n",
        "        ## the equation is (q * k^T)/sqrt(d_model)\n",
        "        ## NOTE: It seems most people use \"reverse indexing\" for the dimensions when transposing k\n",
        "        ## k.transpose(dim0, dim1) will transpose k by swapping dim0 and dim1\n",
        "        ## In standard matrix notation, we would want to swap rows (dim=0) with columns (dim=1)\n",
        "        ## If we have 3 dimensions, because of batching, and the batch was the first dimension\n",
        "        ## And thus dims are defined batch = 0, rows = 1, columns = 2\n",
        "        ## then dim0=-2 = 3 - 2 = 1. dim1=-1 = 3 - 1 = 2.\n",
        "\n",
        "    sims= torch.matmul(q, k.transpose(dim0=self.row_dim, dim1=self.col_dim))\n",
        "    scaled_sims= sims/torch.tensor(k.size(self.col_dim)**0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "      ## Here we are masking out things we don't want to pay attention to,\n",
        "        ## like tokens that come after the current token.\n",
        "      scaled_sims= scaled_sims.masked_fill(mask==0,value=-1e9)\n",
        "\n",
        "    ## Apply softmax to determine what percent of each token's value to\n",
        "        ## use in the final attention values.\n",
        "    attention_percents= F.softmax(scaled_sims, dim=self.col_dim)\n",
        "\n",
        "    # Scale the values by their associated percentages and add them up\n",
        "    attention_scores=torch.matmul(attention_percents, v)\n",
        "    return attention_scores"
      ],
      "metadata": {
        "id": "4DW91hmIbUGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Decoder Class"
      ],
      "metadata": {
        "id": "w9BJTZDXC3R4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A Decoder-Only Transformer simply brings together...\n",
        "\n",
        "Word Embedding\n",
        "\n",
        "Position Encoding\n",
        "\n",
        "Masked Self-Attention\n",
        "\n",
        "Residual Connections\n",
        "\n",
        "A fully connected layer\n",
        "\n",
        "SoftMax - However, the loss function we are using nn.CrossEntropyLoss(), applies the SoftMax for us."
      ],
      "metadata": {
        "id": "2M6vUqCtDMRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderOnlyTransformer(L.LightningModule):\n",
        "\n",
        "    def __init__(self, num_tokens=4, d_model=2, max_len=6):\n",
        "        super().__init__()\n",
        "        L.seed_everything(seed=42)\n",
        "        self.we = nn.Embedding(num_embeddings=num_tokens,\n",
        "                               embedding_dim=d_model)\n",
        "        self.pe = PositionEncoding(d_model=d_model,\n",
        "                                   max_len=max_len)\n",
        "        self.self_attention = Attention(d_model=d_model)\n",
        "        ## NOTE: In this simple example, we are not doing multi-head attention\n",
        "        ## If we wanted to do multi-head attention, we could\n",
        "        ## initailize more Attention objects like this...\n",
        "        ## self.self_attention_2 = Attention(d_model=d_model)\n",
        "        ## self.self_attention_3 = Attention(d_model=d_model)\n",
        "        ## If d_model=2, then using 3 self_attention objects would\n",
        "        ## result in d_model*3 = 6 self-attention values per token,\n",
        "        ## so we would need to initialize\n",
        "        ## a fully connected layer to reduce the dimension of the\n",
        "        ## self attention values back down to d_model like this:\n",
        "        ## self.reduce_attention_dim = nn.Linear(in_features=(num_attention_heads*d_model), out_features=d_model)\n",
        "\n",
        "        self.fc_layer = nn.Linear(in_features=d_model, out_features=num_tokens)\n",
        "        self.loss = nn.CrossEntropyLoss()\n",
        "\n",
        "\n",
        "    def forward(self, token_ids):\n",
        "      ### For the decoder-only transformer, we need to use \"masked self-attention\" so that\n",
        "        ## when we are training we can't cheat and look ahead at\n",
        "        ## what words come after the current word.\n",
        "        ## To create the mask we are creating a matrix where the lower triangle\n",
        "        ## is filled with 0, and everything above the diagonal is filled with 0s.\n",
        "\n",
        "        word_embeddings = self.we(token_ids)\n",
        "        position_encoded = self.pe(word_embeddings)\n",
        "        mask = torch.tril(torch.ones((token_ids.size(dim=0), token_ids.size(dim=0)), device=self.device))\n",
        "        mask = mask == 0\n",
        "        self_attention_values = self.self_attention(position_encoded,\n",
        "                                                    position_encoded,\n",
        "                                                    position_encoded,\n",
        "                                                    mask=mask)\n",
        "        residual_connection_values = position_encoded + self_attention_values\n",
        "        fc_layer_output = self.fc_layer(residual_connection_values)\n",
        "        return fc_layer_output\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        ## configure_optimizers() simply passes the parameters we want to\n",
        "        ## optimize to the optimzes and sets the learning rate\n",
        "        return Adam(self.parameters(), lr=0.1)\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        ## training_step() is called by Lightning trainer when\n",
        "        ## we want to train the model.\n",
        "        input_tokens, labels = batch # collect input\n",
        "        output = self.forward(input_tokens[0])\n",
        "        loss = self.loss(output, labels[0])\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "eGhnxv9wo387"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the transformer, we run an input phrase, either what is computer science <**EOS**> or computer science is what **<EOS>**, through the transformer to get the next predicted token. If the next predicted token is not <**EOS**>, then we add the predicted token to the input tokens and run that through the transformer and repeat until we get the **<EOS>** token or reach the maximum sequence length."
      ],
      "metadata": {
        "id": "XiUvOIThEFzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## First, create a model from DecoderOnlyTransformer()\n",
        "model= DecoderOnlyTransformer(num_tokens=len(token_to_id),d_model=2, max_len=6)\n",
        "\n",
        "## Now create the input for the transformer.\n",
        "model_input= torch.tensor([token_to_id[\"what\"],\n",
        "                            token_to_id[\"is\"],\n",
        "                            token_to_id[\"computer science\"],\n",
        "                            token_to_id[\"<EOS>\"]])\n",
        "input_length= model_input.size(dim=0)\n",
        "\n",
        "## Now get get predictions from the model\n",
        "## NOTE: \"predictions\" is the output from the fully connected layer,\n",
        "##not a softmax() function. We could, if we wanted to,\n",
        "##Run \"predictions\" through a softmax() function, but\n",
        "## since we're going to select the item with the largest value\n",
        "## we can just use argmax instead.\n",
        "predictions=model(model_input)\n",
        "\n",
        "## We'll store predicted_id in an array, predicted_ids, that\n",
        "## we'll add to each time we predict a new output token.\n",
        "predicted_id= torch.tensor([torch.argmax(predictions[-1,:])])\n",
        "predicted_ids= predicted_id\n",
        "\n",
        "max_length=6\n",
        "## Now use a loop to predict output tokens until we get an\n",
        "## <EOS> token.\n",
        "for i in range(input_length, max_length):\n",
        "  if (predicted_id==token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
        "    break\n",
        "  model_input= torch.cat((model_input, predicted_id))\n",
        "  predictions=model(model_input)\n",
        "  predicted_id= torch.tensor([torch.argmax(predictions[-1,:])])\n",
        "  predicted_ids= torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "## Now printout the predicted output phrase.\n",
        "print(\"Predicted Tokens:\\n\")\n",
        "for id in predicted_ids:\n",
        "    print(\"\\t\", id_to_token[id.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRUT_U3hbUBu",
        "outputId": "3480c755-c6e0-4439-c235-5900b0a3350f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:lightning_fabric.utilities.seed:Seed set to 42\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Tokens:\n",
            "\n",
            "\t <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### And, without training, the transformer predicts EOS, but we wanted it to predict awesome EOS So, since the transformer didn't correctly respond to the prompt, we'll have to train it\n"
      ],
      "metadata": {
        "id": "bYRq1-jpFNZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer= L.Trainer(max_epochs=100)\n",
        "trainer.fit(model, dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433,
          "referenced_widgets": [
            "62cf2d2051c64df1a7bb95ad6ea6e11f",
            "577ef759739246fbb0b426d70c2f3c42",
            "6fb4a51d16a44d49adcc8e0fab7abb02",
            "eb355259ec2f42c2abe88041b6b0fadd",
            "4c5f9e6ceb5b4960949ae016e2c101bf",
            "8ef532e321f9448c906656534b9e53ec",
            "85aa9d3da008404e86dbc6c1866135d5",
            "93c8c4f339504542b441e547849a1101",
            "b09bb89a5d7d4f548d57ef7f18e10eb8",
            "f3bcb9d0f42f4075a00eda9b550f3962",
            "e85b62f7ca40451492eb2564295ab9c8"
          ]
        },
        "id": "sfhPhRP1fSNa",
        "outputId": "f8064fff-e53f-44c6-c797-dc2ef02c587d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:ðŸ’¡ Tip: For seamless cloud uploads and versioning, try installing [litmodels](https://pypi.org/project/litmodels/) to enable LitModelCheckpoint, which syncs automatically with the Lightning model registry.\n",
            "INFO:pytorch_lightning.utilities.rank_zero:GPU available: False, used: False\n",
            "INFO:pytorch_lightning.utilities.rank_zero:TPU available: False, using: 0 TPU cores\n",
            "INFO:pytorch_lightning.utilities.rank_zero:HPU available: False, using: 0 HPUs\n",
            "INFO:pytorch_lightning.callbacks.model_summary:\n",
            "  | Name           | Type             | Params | Mode \n",
            "------------------------------------------------------------\n",
            "0 | we             | Embedding        | 10     | train\n",
            "1 | pe             | PositionEncoding | 0      | train\n",
            "2 | self_attention | Attention        | 12     | train\n",
            "3 | fc_layer       | Linear           | 15     | train\n",
            "4 | loss           | CrossEntropyLoss | 0      | train\n",
            "------------------------------------------------------------\n",
            "37        Trainable params\n",
            "0         Non-trainable params\n",
            "37        Total params\n",
            "0.000     Total estimated model params size (MB)\n",
            "8         Modules in train mode\n",
            "0         Modules in eval mode\n",
            "/usr/local/lib/python3.11/dist-packages/pytorch_lightning/loops/fit_loop.py:310: The number of training batches (2) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Training: |          | 0/? [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62cf2d2051c64df1a7bb95ad6ea6e11f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:pytorch_lightning.utilities.rank_zero:`Trainer.fit` stopped: `max_epochs=100` reached.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've trained the transformer, let's use it!"
      ],
      "metadata": {
        "id": "svxszs-YFeVs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the transformer that we just trained, we just repeat what we did earlier, only this time we use the trained transformer instead of an untrained transformer. First, we'll see if it correctly responds to the prompt Computer science is what?"
      ],
      "metadata": {
        "id": "b3AunQgMFhG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Now let's ask the other question...\n",
        "model_input = torch.tensor([token_to_id[\"computer science\"],\n",
        "                            token_to_id[\"is\"],\n",
        "                            token_to_id[\"what\"],\n",
        "                            token_to_id[\"<EOS>\"]])\n",
        "input_length = model_input.size(dim=0)\n",
        "\n",
        "predictions = model(model_input)\n",
        "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
        "predicted_ids = predicted_id\n",
        "\n",
        "for i in range(input_length, max_length):\n",
        "    if (predicted_id == token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
        "        break\n",
        "\n",
        "    model_input = torch.cat((model_input, predicted_id))\n",
        "\n",
        "    predictions = model(model_input)\n",
        "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
        "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "print(\"Predicted Tokens:\\n\")\n",
        "for id in predicted_ids:\n",
        "    print(\"\\t\", id_to_token[id.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtCkl9fap1C1",
        "outputId": "7e1b1dae-0e88-4181-ae0c-222eb01d05eb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Tokens:\n",
            "\n",
            "\t <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We got the correct output! Now let's see if it correctly responds to the prompt what is computer science?"
      ],
      "metadata": {
        "id": "WeQZxh2xFpUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_input = torch.tensor([token_to_id[\"what\"],\n",
        "                            token_to_id[\"is\"],\n",
        "                            token_to_id[\"computer science\"],\n",
        "                            token_to_id[\"<EOS>\"]])\n",
        "input_length = model_input.size(dim=0)\n",
        "\n",
        "predictions = model(model_input)\n",
        "predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
        "predicted_ids = predicted_id\n",
        "\n",
        "for i in range(input_length, max_length):\n",
        "    if (predicted_id == token_to_id[\"<EOS>\"]): # if the prediction is <EOS>, then we are done\n",
        "        break\n",
        "\n",
        "    model_input = torch.cat((model_input, predicted_id))\n",
        "\n",
        "    predictions = model(model_input)\n",
        "    predicted_id = torch.tensor([torch.argmax(predictions[-1,:])])\n",
        "    predicted_ids = torch.cat((predicted_ids, predicted_id))\n",
        "\n",
        "print(\"Predicted Tokens:\\n\")\n",
        "for id in predicted_ids:\n",
        "    print(\"\\t\", id_to_token[id.item()])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0F3-YVJp1Ad",
        "outputId": "e271fdfb-f684-4796-8a34-9f1f59bd0bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Tokens:\n",
            "\n",
            "\t <EOS>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And the output for both questions is awesome EOS, which is exactly what we want."
      ],
      "metadata": {
        "id": "mRPOZmhhFvcO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Hajm0fYQNGy3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}